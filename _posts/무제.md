---
title: "[daily] 6개월 된 개발자"
date: 2021-05-15 00:00:00
categories:
- daily
tags: [daily]
---



살짝 중고이긴 하지만 신입 개발자로 회사에 들어온지 6개월이 됐습니다. 아주 짧은 시간이지만 느낀 점을 남깁니다.



## 역할의 분리

입사 후 첫 업무는 전사에서 아무도 관리하지 않던 사용자 로그를 ETL해 사용할 수 있는 형태로 만드는 것이었습니다. 당시에는 일단 결과물이 나와야한다는 생각밖에 없었기 때문에 필요하면 필요한대로 코드와 메서드를 집어넣었습니다. 

그 결과 지금까지도 그 코드 수정 및 데이터 재적재를 하고 있고, 하나의 단계에서 하나의 역할을 완전히 해야한다는 교훈을 얻었습니다.



`Raw Data -> json -> normalize -> pandas dataframe -> validation -> s3 export`

현재 ETL의 flow입니다. 받아온 json을 pandas normalize 기능을 이용해 1차원 형태로 평평한 pandas dataframe으로 만들고 데이터 검증을 한 뒤 s3로 다시 올리는 간단한 flow입니다.



그런데 만약 다시 처음부터 flow를 만든다면 이렇게 할 것 같습니다.

`Raw Data -> json -> validation -> pandas dataframe -> s3 export`



validation의 단계를 한 칸 앞으로 당겼습니다. 그렇지만 얻을 수 있는 기대효과는 아주 큽니다. 그 이유는 다음과 같습니다.

현재 내려받고 있는 raw data(json)의 형태는 다음과 같습니다.

```json
{a: 1, b: 2, c: {d: 4, e: 5}}
{a: 1, b: 2, c: {d: 4, e: 5}, f: 4}
...
...
```

완전한 형태의 json이 아니라 줄바꿈을 기준으로 나눠져 있는 json의 뭉치를 내려받습니다. 이걸 splitlines 메서드로 쫙 편 뒤 최소한의 검증을 거치고 pandas의 normalize 메서드로 pandas dataframe으로 만든 뒤 대규모 validation 로직을 가지고 dataframe의 row나 column을 삭제하거나 변경합니다.

이 방법의 가장 큰 단점은 dataframe 단계의 로직을 변경하면 json 단계의 로직까지 전부 신경써야 한다는 점입니다. 만약 json 단계에서 json validator를 이용해 불필요한 json을 전부 쳐내고 데이터 타입에 맞는 json만 dataframe으로 만들었다면 dataframe 단계에서는 s3 export 말고는 하는 일이 없었을 것입니다. validation에 대한 기능을 json 단계에서 전부 진행함으로써 역할의 분리가 확실해지는 것입니다.

그런데 뒷단계인 dataframe 단계에서도 transform을 하고, 앞단계인 json 단계에서도 normalize을 진행하니 어느 한쪽을 쉽게 건드릴 수가 없는 상황이 되어버렸습니다. 데이터를 받아오는 최전방에서 validation을 했다면 벌어지지 않았을 상황이었는데 이 점이 아쉽습니다.





